---
layout: post
title: Perché è necessario spiegare le decisioni di un sistema di Intelligenza Artificiale?
bigimg: /img/web-3706562_1920.jpg
---

Forse in molti non se ne rendono ancora conto, ma **l'Intelligenza Artificiale sta cambiando ogni aspetto delle nostre vite**. La mattina, prima di uscire di casa, lasciamo decidere a Google Maps quale sia il tratto di strada migliore da percorrere per arrivare in tempo a scuola, all'università oppure a lavoro. Ci fidiamo del nostro smartphone come di nessun altro: gli lasciamo decidere le cose da comprare, le canzoni da ascoltare, i film da guardare. Lui, d'altro canto, conosce i nostri gusti come nessun altro, e ce lo dimostra tramite delle pubblicità mirate che ci invitano ad acquistare prodotti di cui abbiamo appena parlato coi nostri amici e che probabilmente acquisteremo pur non avendone effettivamente bisogno. 

Allo stato attuale, non bisogna pensare ai sistemi di AI come se fossero degli umanoidi con una vera e propria intelligenza e coscienza di sé --- come Ava in *Ex Machina*, famoso film del 2015 scritto e diretto da Alex Garland ---, in quanto si tratta per lo più di **sistemi software** che, addestrati per un unico task, **applicano varie formule matematiche per prendere delle decisioni sulla base dei dati che gli vengono forniti**. 

Nella maggior parte dei casi, tali sistemi sono complessi al punto tale da non permettere all'utilizzatore (nemmeno ai loro stessi creatori!) di capire quale sia stato il processo che ha portato a una particolare decisione. Essi sono, dunque, come delle **enormi scatole nere** (in gergo, *black-box*) che trasformano in decisioni ciò che mettiamo al loro interno, ma non ci forniscono alcuna spiegazione.

![AI as a black-box](/img/img20200126_12021114.png)

Un [articolo su New Scientist](https://www.newscientist.com/article/2222907-ai-can-predict-if-youll-die-soon-but-weve-no-idea-how-it-works/) sottolinea le enormi potenzialità dell'Intelligenza Artificiale: un algoritmo è in grado di ottenere prestazioni migliori rispetto ai medici nella rilevazione di pazienti ad alto rischio sulla base dei loro elettrocardiogrammi (ECG) **riuscendo, in qualche modo, a carpire dai dati qualcosa che all'umano sfugge**. 

Questo semplice esempio ci consente di ragionare sul fatto che, sebbene l'Intelligenza Artificiale costituisca un'enorme potenzialità in disparati contesti, **la sua applicabilità è ostacolata dall'"opacità" dei modelli**, soprattutto nei casi in cui le decisioni prese hanno un notevole impatto sulle vite delle persone. Un medico, per esempio, nel fornire una diagnosi, non si fiderà mai dell'aiuto di una macchina senza una solida base scientifica. 

La necessità di un'AI spiegabile non nasce solo dal desiderio di permettere all'uomo di imparare dalle macchine, ma anche e soprattutto dal principio di *responsabilità* delle azioni: se, per esempio, un'auto a guida autonoma, senza un conducente al suo interno, investisse un pedone ([ed è successo!](https://www.theguardian.com/technology/2018/mar/19/uber-self-driving-car-kills-woman-arizona-tempe)), a chi verrebbe inputata la colpa? È per tale motivo che **i governi in tutto il mondo stanno cominciando a regolare l'utilizzo dell'Intelligenza Artificiale, affinché le applicazioni rispettino determinati principi, tra i quali l'interpretabilità delle decisioni**. A titolo esemplificativo, di seguito è riportatata parte dell'[articolo 22](https://www.cyberlaws.it/2017/articolo-22-gdpr-regolamento-generale-sulla-protezione-dei-dati-ue2016679/) del GDPR (*General Data Protection Regulation*):

> L’interessato ha il diritto di non essere sottoposto a una decisione basata unicamente sul trattamento automatizzato, compresa la profilazione, che produca effetti giuridici che lo riguardano o che incida in modo analogo significativamente sulla sua persona.
